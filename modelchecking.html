<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>7. Model checking and diagnostics &mdash; PyMC 2.3.6 documentation</title>
    
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '2.3.6',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="PyMC 2.3.6 documentation" href="index.html" />
    <link rel="next" title="8. Extending PyMC" href="extending.html" />
    <link rel="prev" title="6. Saving and managing sampling results" href="database.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="extending.html" title="8. Extending PyMC"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="database.html" title="6. Saving and managing sampling results"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">PyMC 2.3.6 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="model-checking-and-diagnostics">
<span id="chap-modelchecking"></span><h1>7. Model checking and diagnostics<a class="headerlink" href="#model-checking-and-diagnostics" title="Permalink to this headline">¶</a></h1>
<div class="section" id="convergence-diagnostics">
<span id="convergence"></span><h2>7.1. Convergence Diagnostics<a class="headerlink" href="#convergence-diagnostics" title="Permalink to this headline">¶</a></h2>
<p>Valid inferences from sequences of MCMC samples are based on the assumption
that the samples are derived from the true posterior distribution of interest.
Theory guarantees this condition as the number of iterations approaches
infinity. It is important, therefore, to determine the minimum number of
samples required to ensure a reasonable approximation to the target posterior
density. Unfortunately, no universal threshold exists across all problems, so
convergence must be assessed independently each time MCMC estimation is
performed. The procedures for verifying convergence are collectively known as
convergence diagnostics.</p>
<p>One approach to analyzing convergence is analytical, whereby the variance of
the sample at different sections of the chain are compared to that of the
limiting distribution. These methods use distance metrics to analyze
convergence, or place theoretical bounds on the sample variance, and though
they are promising, they are generally difficult to use and are not prominent
in the MCMC literature. More common is a statistical approach to assessing
convergence. With this approach, rather than considering the properties of the
theoretical target distribution, only the statistical properties of the
observed chain are analyzed. Reliance on the sample alone restricts such
convergence criteria to heuristics. As a result, convergence cannot be guaranteed.
Although evidence for lack of convergence using statistical convergence
diagnostics will correctly imply lack of convergence in the chain, the absence
of such evidence will not <em>guarantee</em> convergence in the chain. Nevertheless,
negative results for one or more criteria may provide some measure of
assurance to users that their sample will provide valid inferences.</p>
<p>For most simple models, convergence will occur quickly, sometimes within a the
first several hundred iterations, after which all remaining samples of the
chain may be used to calculate posterior quantities. For more complex
models, convergence requires a significantly longer burn-in period; sometimes
orders of magnitude more samples are needed. Frequently, lack of convergence
will be caused by poor mixing (Figure 7.1). Recall that <em>mixing</em> refers
to the degree to which the Markov chain explores the support of the posterior
distribution. Poor mixing may stem from inappropriate proposals (if one is
using the Metropolis-Hastings sampler) or from attempting to estimate models
with highly correlated variables.</p>
<div class="figure align-center" id="id13">
<span id="mix"></span><a class="reference internal image-reference" href="_images/poormixing.png"><img alt="Poor mixing figure" src="_images/poormixing.png" /></a>
<p class="caption"><span class="caption-text">An example of a poorly-mixing sample in two dimensions. Notice that the
chain is trapped in a region of low probability relative to the mean
(dot) and variance (oval) of the true posterior quantity.</span></p>
</div>
<div class="section" id="informal-methods">
<h3>7.1.1. Informal Methods<a class="headerlink" href="#informal-methods" title="Permalink to this headline">¶</a></h3>
<p>The most straightforward approach for assessing convergence is based on simply
plotting and inspecting traces and histograms of the observed MCMC sample. If
the trace of values for each of the stochastics exhibits asymptotic behavior
<a class="footnote-reference" href="#id12" id="id1">[1]</a> over the last <span class="math">\(m\)</span> iterations, this may be satisfactory evidence for
convergence. A similar approach involves plotting a histogram for every set of
<span class="math">\(k\)</span> iterations (perhaps 50-100) beyond some burn in threshold <span class="math">\(n\)</span>;
if the histograms are not visibly different among the sample intervals, this is
reasonable evidence for convergence. Note that such diagnostics should be
carried out for each stochastic estimated by the MCMC algorithm, because
convergent behavior by one variable does not imply evidence for convergence for
other variables in the analysis. An extension of this approach can be taken
when multiple parallel chains are run, rather than just a single, long chain.
In this case, the final values of <span class="math">\(c\)</span> chains run for <span class="math">\(n\)</span> iterations
are plotted in a histogram; just as above, this is repeated every <span class="math">\(k\)</span>
iterations thereafter, and the histograms of the endpoints are plotted again
and compared to the previous histogram. This is repeated until consecutive
histograms are indistinguishable.</p>
<p>Another <em>ad hoc</em> method for detecting lack of convergence is to examine the
traces of several MCMC chains initialized with different starting values.
Overlaying these traces on the same set of axes should (if convergence has
occurred) show each chain tending toward the same equilibrium value, with
approximately the same variance. Recall that the tendency for some Markov
chains to converge to the true (unknown) value from diverse initial values is
called <em>ergodicity</em>. This property is guaranteed by the reversible chains
constructed using MCMC, and should be observable using this technique. Again,
however, this approach is only a heuristic method, and cannot always detect
lack of convergence, even though chains may appear ergodic.</p>
<div class="figure align-center" id="id14">
<span id="metas"></span><a class="reference internal image-reference" href="_images/metastable.png"><img alt="_images/metastable.png" src="_images/metastable.png" /></a>
<p class="caption"><span class="caption-text">An example of metastability in a two-dimensional parameter space. The
chain appears to be stable in one region of the parameter space for an
extended period, then unpredictably jumps to another region of the
space.</span></p>
</div>
<p>A principal reason that evidence from informal techniques cannot guarantee
convergence is a phenomenon called metastability. Chains may appear to have
converged to the true equilibrium value, displaying excellent qualities by any
of the methods described above. However, after some period of stability around
this value, the chain may suddenly move to another region of the parameter
space (Figure 7.2). This period of metastability can sometimes be very
long, and therefore escape detection by these convergence diagnostics.
Unfortunately, there is no statistical technique available for detecting
metastability.</p>
</div>
<div class="section" id="formal-methods">
<h3>7.1.2. Formal Methods<a class="headerlink" href="#formal-methods" title="Permalink to this headline">¶</a></h3>
<p>Along with the <em>ad hoc</em> techniques described above, a number of more formal
methods exist which are prevalent in the literature. These are considered more
formal because they are based on existing statistical methods, such as time
series analysis.</p>
<p>PyMC currently includes three formal convergence diagnostic methods. The first,
proposed by <a class="reference internal" href="references.html#geweke1992" id="id2">[Geweke1992]</a>, is a time-series approach that compares the mean
and variance of segments from the beginning and end of a single chain.</p>
<div class="math">
\[z = \frac{\bar{\theta}_a - \bar{\theta}_b}{\sqrt{Var(\theta_a) + Var(\theta_b)}}\]</div>
<p>where <span class="math">\(a\)</span> is the early interval and <span class="math">\(b\)</span> the late interval. If the
z-scores (theoretically distributed as standard normal variates) of these two
segments are similar, it can provide evidence for convergence. PyMC calculates
z-scores of the difference between various initial segments along the chain,
and the last 50% of the remaining chain. If the chain has converged, the
majority of points should fall within 2 standard deviations of zero.</p>
<p>Diagnostic z-scores can be obtained by calling the <code class="docutils literal"><span class="pre">geweke</span></code> function. It
accepts either (1) a single trace, (2) a Node or Stochastic object, or (4) an
entire Model object:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">geweke</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">first</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">last</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">intervals</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
<p>The arguments expected are the following:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">pymc_object</span></code>: The object that is or contains the output trace(s).</li>
<li><code class="docutils literal"><span class="pre">first</span></code> (optional): First portion of chain to be used in Geweke diagnostic.
Defaults to 0.1 (<em>i.e.</em> first 10% of chain).</li>
<li><code class="docutils literal"><span class="pre">last</span></code> (optional): Last portion of chain to be used in Geweke diagnostic.
Defaults to 0.5 (<em>i.e.</em> last 50% of chain).</li>
<li><code class="docutils literal"><span class="pre">intervals</span></code> (optional): Number of sub-chains to analyze. Defaults to 20.</li>
</ul>
<p>The resulting scores are best interpreted graphically, using the
<code class="docutils literal"><span class="pre">geweke_plot</span></code> function. This displays the scores in series, in relation to
the 2 standard deviation boundaries around zero. Hence, it is easy to see
departures from the standard normal assumption.</p>
<div class="figure align-center" id="id15">
<span id="geweke"></span><a class="reference internal image-reference" href="_images/geweke.png"><img alt="Geweke figure." src="_images/geweke.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-text">Sample plot of Geweke z-scores for a variable using <code class="docutils literal"><span class="pre">geweke_plot</span></code>.
The occurrence of the scores well within 2 standard deviations of zero
gives not indicate of lack of convergence (top), while deviations exceeding
2 standard deviations suggest that additional samples are requred to
achieve convergence (bottom).</span></p>
</div>
<p><code class="docutils literal"><span class="pre">geweke_plot</span></code> takes either a single set of scores, or a dictionary of scores
(output by <code class="docutils literal"><span class="pre">geweke</span></code> when an entire Sampler is passed) as its argument:</p>
<div class="highlight-python"><div class="highlight"><pre>def geweke_plot(scores, name=&#39;geweke&#39;, format=&#39;png&#39;, suffix=&#39;-diagnostic&#39;,
                path=&#39;./&#39;, fontmap = {1:10, 2:8, 3:6, 4:5, 5:4}, verbose=1)
</pre></div>
</div>
<p>The arguments are defined as:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">scores</span></code>: The object that contains the Geweke scores. Can be a list (one
set) or a dictionary (multiple sets).</li>
<li><code class="docutils literal"><span class="pre">name</span></code> (optional): Name used for output files. For multiple scores, the
dictionary keys are used as names.</li>
<li><code class="docutils literal"><span class="pre">format</span></code> (optional): Graphic output file format (defaults to <em>png</em>).</li>
<li><code class="docutils literal"><span class="pre">suffix</span></code> (optional): Suffix to filename (defaults to <em>-diagnostic</em>)</li>
<li><code class="docutils literal"><span class="pre">path</span></code> (optional): The path for output graphics (defaults to working
directory).</li>
<li><code class="docutils literal"><span class="pre">fontmap</span></code> (optional): Dictionary containing the font map for the labels of
the graphic.</li>
<li><code class="docutils literal"><span class="pre">verbose</span></code> (optional): Verbosity level for output (defaults to 1).</li>
</ul>
<p>To illustrate, consider the sample model <code class="docutils literal"><span class="pre">gelman_bioassay</span></code> that is used to
instantiate a MCMC sampler. The sampler is then run for a given number of
iterations:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pymc.examples</span> <span class="kn">import</span> <span class="n">gelman_bioassay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="n">pymc</span><span class="o">.</span><span class="n">MCMC</span><span class="p">(</span><span class="n">gelman_bioassay</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">burn</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
<p>It is easiest simply to pass the entire sampler <code class="docutils literal"><span class="pre">S</span></code> the <code class="docutils literal"><span class="pre">geweke</span></code> function:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">pymc</span><span class="o">.</span><span class="n">geweke</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">intervals</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pymc</span><span class="o">.</span><span class="n">Matplot</span><span class="o">.</span><span class="n">geweke_plot</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, individual stochastics within <code class="docutils literal"><span class="pre">S</span></code> can be analyzed for
convergence:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">trace</span> <span class="o">=</span> <span class="n">S</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="s">&quot;alpha&quot;</span><span class="p">)[:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alpha_scores</span> <span class="o">=</span> <span class="n">pymc</span><span class="o">.</span><span class="n">geweke</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">intervals</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pymc</span><span class="o">.</span><span class="n">Matplot</span><span class="o">.</span><span class="n">geweke_plot</span><span class="p">(</span><span class="n">alpha_scores</span><span class="p">,</span> <span class="s">&quot;alpha&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>An example of convergence and non-convergence of a chain using <cite>geweke_plot</cite> is
given in Figure 7.3.</p>
<p>The second diagnostic provided by PyMC is the <a class="reference internal" href="references.html#raftery1995a" id="id3">[Raftery1995a]</a> procedure. This
approach estimates the number of iterations required to reach convergence,
along with the number of burn-in samples to be discarded and the appropriate
thinning interval. A separate estimate of both quantities can be obtained for
each variable in a given model.</p>
<p>As the criterion for determining convergence, the Raftery and Lewis approach
uses the accuracy of estimation of a user-specified quantile. For example, we
may want to estimate the quantile <span class="math">\(q=0.975\)</span> to within <span class="math">\(r=0.005\)</span>
with probability <span class="math">\(s=0.95\)</span>. In other words,</p>
<div class="math">
\[Pr(|\hat{q}-q| \le r) = s\]</div>
<p>From any sample of <span class="math">\(\theta\)</span>, one can construct a binary chain:</p>
<div class="math">
\[Z^{(j)} = I(\theta^{(j)} \le u_q)\]</div>
<p>where <span class="math">\(u_q\)</span> is the quantile value and <span class="math">\(I\)</span> is the indicator
function. While <span class="math">\(\{\theta^{(j)}\}\)</span> is a Markov chain, <span class="math">\(\{Z^{(j)}\}\)</span>
is not necessarily so. In any case, the serial dependency among <span class="math">\(Z^{(j)}\)</span>
decreases as the thinning interval <span class="math">\(k\)</span> increases. A value of <span class="math">\(k\)</span> is
chosen to be the smallest value such that the first order Markov chain is
preferable to the second order Markov chain.</p>
<p>This thinned sample is used to determine number of burn-in samples. This is
done by comparing the remaining samples from burn-in intervals of increasing
length to the limiting distribution of the chain. An appropriate value is one
for which the truncated sample&#8217;s distribution is within <span class="math">\(\epsilon\)</span>
(arbitrarily small) of the limiting distribution. See <a class="reference internal" href="references.html#raftery1995a" id="id4">[Raftery1995a]</a> or
<a class="reference internal" href="references.html#gamerman1997" id="id5">[Gamerman1997]</a> for computational details. Estimates for sample size tend to
be conservative.</p>
<p>This diagnostic is best used on a short pilot run of a particular model, and
the results used to parameterize a subsequent sample that is to be used for
inference. Its calling convention is as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">raftery_lewis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s</span><span class="o">=.</span><span class="mi">95</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=.</span><span class="mo">001</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>The arguments are:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">pymc_object</span></code>: The object that contains the Geweke scores. Can be a list
(one set) or a dictionary (multiple sets).</li>
<li><code class="docutils literal"><span class="pre">q</span></code>: Desired quantile to be estimated.</li>
<li><code class="docutils literal"><span class="pre">r</span></code>: Desired accuracy for quantile.</li>
<li><code class="docutils literal"><span class="pre">s</span></code> (optional): Probability of attaining the requested accuracy (defaults
to 0.95).</li>
<li><code class="docutils literal"><span class="pre">epsilon</span></code> (optional) : Half width of the tolerance interval required for
the q-quantile (defaults to 0.001).</li>
<li><code class="docutils literal"><span class="pre">verbose</span></code> (optional) : Verbosity level for output (defaults to 1).</li>
</ul>
<p>The code for <code class="docutils literal"><span class="pre">raftery_lewis</span></code> is based on the FORTRAN program <em>gibbsit</em>
(<a class="reference internal" href="references.html#raftery1995b" id="id6">[Raftery1995b]</a>).</p>
<p>For example, consider again a sampler S run for some model my_model:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="n">pymc</span><span class="o">.</span><span class="n">MCMC</span><span class="p">(</span><span class="n">my_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">burn</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
<p>One can pass either the entire sampler S or any stochastic within S to the
<cite>raftery_lewis</cite> function, along with suitable arguments. Here, we have chosen
<span class="math">\(q = 0.025\)</span> (the lower limit of the equal-tailed 95% interval) and error
<span class="math">\(r = 0.01\)</span>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">pymc</span><span class="o">.</span><span class="n">raftery_lewis</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
<p>This yields diagnostics as follows for each stochastic of S, as well as a
dictionary containing the diagnostic quantities:</p>
<div class="highlight-python"><div class="highlight"><pre>========================
Raftery-Lewis Diagnostic
========================

937 iterations required (assuming independence) to achieve 0.01 accuracy
with 95 percent probability.

Thinning factor of 1 required to produce a first-order Markov chain.

39 iterations to be discarded at the beginning of the simulation (burn-in).

11380 subsequent iterations required.

Thinning factor of 11 required to produce an independence chain.
</pre></div>
</div>
<p>The third convergence diagnostic provided by PyMC is the Gelman-Rubin statistic
(<a class="reference internal" href="references.html#gelman1992" id="id7">[Gelman1992]</a>). This diagnostic uses multiple chains to check for lack of
convergence, and is based on the notion that if multiple chains have converged,
by definition they should appear very similar to one another; if not, one or
more of the chains has failed to converge.</p>
<p>The Gelman-Rubin diagnostic uses an analysis of variance approach to assessing
convergence. That is, it calculates both the between-chain varaince (B) and
within-chain varaince (W), and assesses whether they are different enough to
worry about convergence. Assuming <span class="math">\(m\)</span> chains, each of length <span class="math">\(n\)</span>,
quantities are calculated by:</p>
<div class="math">
\[\begin{split}B &amp;= \frac{n}{m-1} \sum_{j=1}^m (\bar{\theta}_{.j} - \bar{\theta}_{..})^2 \\
W &amp;= \frac{1}{m} \sum_{j=1}^m \left[ \frac{1}{n-1} \sum_{i=1}^n (\theta_{ij} - \bar{\theta}_{.j})^2 \right]\end{split}\]</div>
<p>for each scalar estimand <span class="math">\(\theta\)</span>. Using these values, an estimate of the
marginal posterior variance of <span class="math">\(\theta\)</span> can be calculated:</p>
<div class="math">
\[\hat{\text{Var}}(\theta | y) = \frac{n-1}{n} W + \frac{1}{n} B\]</div>
<p>Assuming <span class="math">\(\theta\)</span> was initialized to arbitrary starting points in each
chain, this quantity will overestimate the true marginal posterior variance. At
the same time, <span class="math">\(W\)</span> will tend to underestimate the within-chain variance
early in the sampling run. However, in the limit as <span class="math">\(n \rightarrow
\infty\)</span>, both quantities will converge to the true variance of <span class="math">\(\theta\)</span>.
In light of this, the Gelman-Rubin statistic monitors convergence using the
ratio:</p>
<div class="math">
\[\hat{R} = \sqrt{\frac{\hat{\text{Var}}(\theta | y)}{W}}\]</div>
<p>This is called the potential scale reduction, since it is an estimate of the
potential reduction in the scale of <span class="math">\(\theta\)</span> as the number of simulations
tends to infinity. In practice, we look for values of <span class="math">\(\hat{R}\)</span> close to
one (say, less than 1.1) to be confident that a particular estimand has
converged. In PyMC, the function <cite>gelman_rubin</cite> will calculate <span class="math">\(\hat{R}\)</span>
for each stochastic node in the passed model:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">pymc</span><span class="o">.</span><span class="n">gelman_rubin</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="go">{&#39;alpha&#39;: 1.0036389589627821,</span>
<span class="go"> &#39;beta&#39;: 1.001503957313336,</span>
<span class="go"> &#39;theta&#39;: [1.0013923468783055,</span>
<span class="go">  1.0274479503713816,</span>
<span class="go">  0.95365716267969636,</span>
<span class="go">  1.00267321019079]}</span>
</pre></div>
</div>
<p>For the best results, each chain should be initialized to highly dispersed starting values for each stochastic node.</p>
<p>By default, when calling the <code class="docutils literal"><span class="pre">summary_plot</span></code> function using nodes with multiple chains, the <span class="math">\(\hat{R}\)</span> values will be plotted alongside the posterior intervals.</p>
<div class="figure align-center" id="id16">
<span id="summary-plot"></span><a class="reference internal image-reference" href="_images/summary.png"><img alt="_images/summary.png" src="_images/summary.png" style="width: 800px;" /></a>
<p class="caption"><span class="caption-text">Summary plot of parameters from <cite>gelman_bioassay</cite> model, showing credible
intervals on the left and the Gelman-Rubin statistic on the right.</span></p>
</div>
<p>Additional convergence diagnostics are available in the <a class="reference external" href="http://lib.stat.cmu.edu/r/cran/">R</a> statistical
package (<a class="reference internal" href="references.html#r2010" id="id8">[R2010]</a>), via the <a class="reference external" href="http://www-fis.iarc.fr/coda/">CODA</a> module (<a class="reference internal" href="references.html#plummer2008" id="id9">[Plummer2008]</a>). PyMC includes a
method <code class="docutils literal"><span class="pre">coda</span></code> for exporting model traces in a format that may be directly
read by <code class="docutils literal"><span class="pre">coda</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">pymc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">coda</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>

<span class="go">Generating CODA output</span>
<span class="go">==================================================</span>
<span class="go">Processing deaths</span>
<span class="go">Processing beta</span>
<span class="go">Processing theta</span>
<span class="go">Processing alpha</span>
</pre></div>
</div>
<p>The lone argument is the PyMC sampler for which output is desired.</p>
<p>Calling <code class="docutils literal"><span class="pre">coda</span></code> yields a file containing raw trace values (suffix <code class="docutils literal"><span class="pre">.out</span></code>)
and a file containing indices to the trace values (suffix <code class="docutils literal"><span class="pre">.ind</span></code>).</p>
</div>
</div>
<div class="section" id="autocorrelation-plots">
<span id="autocorr-section"></span><h2>7.2. Autocorrelation Plots<a class="headerlink" href="#autocorrelation-plots" title="Permalink to this headline">¶</a></h2>
<p>Samples from MCMC algorithms are ususally autocorrelated, due partly to the
inherent Markovian dependence structure. The degree of autocorrelation can be
quantified using the autocorrelation function:</p>
<div class="math">
\[\begin{split}\rho_k &amp; = \frac{\mbox{Cov}(X_t,  X_{t+k})}{\sqrt{\mbox{Var}(X_t)\mbox{Var}(X_{t+k})}} \\
      &amp; = \frac{E[(X_t - \theta)(X_{t+k} - \theta)]}{\sqrt{E[(X_t - \theta)^2] E[(X_{t+k} - \theta)^2]}}\end{split}\]</div>
<p>PyMC includes a function for plotting the autocorrelation function for each
stochastics in the sampler (Figure 7.5). This allows users to
examine the relationship among successive samples within sampled chains.
Significant autocorrelation suggests that chains require thinning prior to use
of the posterior statistics for inference.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">autocorrelation</span><span class="p">(</span><span class="n">pymc_object</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">maxlag</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s">&#39;png&#39;</span><span class="p">,</span> <span class="n">suffix</span><span class="o">=</span><span class="s">&#39;-acf&#39;</span><span class="p">,</span>
<span class="n">path</span><span class="o">=</span><span class="s">&#39;./&#39;</span><span class="p">,</span> <span class="n">fontmap</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">:</span><span class="mi">4</span><span class="p">},</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">pymc_object</span></code>: The object that is or contains the output trace(s).</li>
<li><code class="docutils literal"><span class="pre">name</span></code>: Name used for output files.</li>
<li><code class="docutils literal"><span class="pre">maxlag</span></code>: The highest lag interval for which autocorrelation is calculated.</li>
<li><code class="docutils literal"><span class="pre">format</span></code> (optional): Graphic output file format (defaults to <em>png</em>).</li>
<li><code class="docutils literal"><span class="pre">suffix</span></code> (optional): Suffix to filename (defaults to <em>-diagnostic</em>)</li>
<li><code class="docutils literal"><span class="pre">path</span></code> (optional): The path for output graphics (defaults to working
directory).</li>
<li><code class="docutils literal"><span class="pre">fontmap</span></code> (optional): Dictionary containing the font map for the labels of
the graphic.</li>
<li><code class="docutils literal"><span class="pre">verbose</span></code> (optional): Verbosity level for output (defaults to 1).</li>
</ul>
<p>Autocorrelation plots can be obtained simply by passing the sampler to the
<cite>autocorrelation</cite> function (within the <cite>Matplot</cite> module) directly:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">S</span> <span class="o">=</span> <span class="n">pymc</span><span class="o">.</span><span class="n">MCMC</span><span class="p">(</span><span class="n">gelman_bioassay</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">S</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="n">burn</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pymc</span><span class="o">.</span><span class="n">Matplot</span><span class="o">.</span><span class="n">autocorrelation</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, variables within a model can be plotted individually. For
example, the parameter <cite>beta</cite> that was estimated using sampler <cite>S</cite> for the
<cite>gelman_bioassay</cite> model will yield a correlation plot as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">pymc</span><span class="o">.</span><span class="n">Matplot</span><span class="o">.</span><span class="n">autocorrelation</span><span class="p">(</span><span class="n">S</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id17">
<span id="autocorr"></span><a class="reference internal image-reference" href="_images/autocorr.png"><img alt="Autocorrelation figure" src="_images/autocorr.png" style="width: 560.0px;" /></a>
<p class="caption"><span class="caption-text">Sample autocorrelation plot for the switchpoint variable from the coal
mining disasters example model.</span></p>
</div>
</div>
<div class="section" id="goodness-of-fit">
<span id="gof-section"></span><h2>7.3. Goodness of Fit<a class="headerlink" href="#goodness-of-fit" title="Permalink to this headline">¶</a></h2>
<p>Checking for model convergence is only the first step in the evaluation of MCMC
model outputs. It is possible for an entirely unsuitable model to converge, so
additional steps are needed to ensure that the estimated model adequately fits
the data. One intuitive way of evaluating model fit is to compare model
predictions with the observations used to fit the model. In other words, the
fitted model can be used to simulate data, and the distribution of the
simulated data should resemble the distribution of the actual data.</p>
<p>Fortunately, simulating data from the model is a natural component of the
Bayesian modelling framework. Recall, from the discussion on imputation of
missing data, the posterior predictive distribution:</p>
<div class="math">
\[p(\tilde{y}|y) = \int p(\tilde{y}|\theta) f(\theta|y) d\theta\]</div>
<p>Here, <span class="math">\(\tilde{y}\)</span> represents some hypothetical new data that would be
expected, taking into account the posterior uncertainty in the model
parameters. Sampling from the posterior predictive distribution is easy in
PyMC. The code looks identical to the corresponding data stochastic, with two
modifications: (1) the node should be specified as deterministic and (2) the
statistical likelihoods should be replaced by random number generators. As an
example, consider a simple dose-response model, where deaths are modeled as a
binomial random variable for which the probability of death is a logit-linear
function of the dose of a particular drug:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">n</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span>
<span class="n">dose</span> <span class="o">=</span> <span class="p">[</span><span class="o">-.</span><span class="mi">86</span><span class="p">,</span><span class="o">-.</span><span class="mi">3</span><span class="p">,</span><span class="o">-.</span><span class="mo">05</span><span class="p">,</span><span class="o">.</span><span class="mi">73</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="n">pymc</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">pymc</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="nd">@pymc.deterministic</span>
<span class="k">def</span> <span class="nf">theta</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">dose</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;theta = inv_logit(a+b)&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">pymc</span><span class="o">.</span><span class="n">invlogit</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">*</span><span class="n">d</span><span class="p">)</span>

<span class="c"># deaths ~ binomial(n, p)</span>
<span class="n">deaths</span> <span class="o">=</span> <span class="n">pymc</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">&#39;deaths&#39;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The posterior predictive distribution of deaths uses the same functional form
as the data likelihood, in this case a binomial stochastic. Here is the
corresponding sample from the posterior predictive distribution:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">deaths_sim</span> <span class="o">=</span> <span class="n">pymc</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s">&#39;deaths_sim&#39;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
<p>Notice that the observed stochastic <cite>pymc.Binomial</cite> has been replaced with a
stochastic node that is identical in every respect to <cite>deaths</cite>, except that its
values are not fixed to be the observed data &#8211; they are left to vary according
to the values of the fitted parameters.</p>
<p>The degree to which simulated data correspond to observations can be evaluated
in at least two ways. First, these quantities can simply be compared visually.
This allows for a qualitative comparison of model-based replicates and
observations. If there is poor fit, the true value of the data may appear in
the tails of the histogram of replicated data, while a good fit will tend to
show the true data in high-probability regions of the posterior predictive
distribution (Figure 7.6).</p>
<div class="figure align-center" id="id18">
<span id="gof"></span><a class="reference internal image-reference" href="_images/gof.png"><img alt="GOF figure" src="_images/gof.png" style="width: 560.0px;" /></a>
<p class="caption"><span class="caption-text">Data sampled from the posterior predictive distribution of a binomial random
variate. The observed value (1) is shown by the dotted red line.</span></p>
</div>
<p>The Matplot package in PyMC provides an easy way of producing such plots, via
the <code class="docutils literal"><span class="pre">gof_plot</span></code> function. To illustrate, consider a single data point <code class="docutils literal"><span class="pre">x</span></code>
and an array of values <code class="docutils literal"><span class="pre">x_sim</span></code> sampled from the posterior predictive
distribution. The histogram is generated by calling:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">pymc</span><span class="o">.</span><span class="n">Matplot</span><span class="o">.</span><span class="n">gof_plot</span><span class="p">(</span><span class="n">x_sim</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;x&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>A second approach for evaluating goodness of fit using samples from the
posterior predictive distribution involves the use of a statistical criterion.
For example, the Bayesian p-value <a class="reference internal" href="references.html#gelman1996" id="id10">[Gelman1996]</a> uses a discrepancy measure
that quantifies the difference between data (observed or simulated) and the
expected value, conditional on some model. One such discrepancy measure is the
Freeman-Tukey statistic <a class="reference internal" href="references.html#brooks2000" id="id11">[Brooks2000]</a>:</p>
<div class="math">
\[D(x|\theta) = \sum_j (\sqrt{x_j}-\sqrt{e_j})^2,\]</div>
<p>where the <span class="math">\(x_j\)</span> are data and <span class="math">\(e_j\)</span> are the corresponding expected
values, based on the model. Model fit is assessed by comparing the
discrepancies from observed data to those from simulated data. On average, we
expect the difference between them to be zero; hence, the Bayesian <em>p</em> value is
simply the proportion of simulated discrepancies that are larger than their
corresponding observed discrepancies:</p>
<div class="math">
\[\begin{split}p = Pr[ D(x_{\text{sim}}|\theta) &gt; D(x_{\text{obs}}|\theta) ]\end{split}\]</div>
<p>If <span class="math">\(p\)</span> is very large (e.g. <span class="math">\(&gt;0.975\)</span>) or very small (e.g.
<span class="math">\(&lt;0.025\)</span>) this implies that the model is not consistent with the data,
and thus is evidence of lack of fit. Graphically, data and simulated
discrepancies plotted together should be clustered along a 45 degree line
passing through the origin, as shown in Figure 7.7.</p>
<div class="figure align-center" id="id19">
<span id="deviate"></span><a class="reference internal image-reference" href="_images/deviates.png"><img alt="deviates figure" src="_images/deviates.png" style="width: 640.0px;" /></a>
<p class="caption"><span class="caption-text">Plot of deviates of observed and simulated data from expected values.
The cluster of points symmetrically about the 45 degree line (and the
reported p-value) suggests acceptable fit for the modeled parameter.</span></p>
</div>
<p>The <code class="docutils literal"><span class="pre">discrepancy</span></code> function in the <code class="docutils literal"><span class="pre">diagnostics</span></code> package can be used to
generate discrepancy statistics from arrays of data, simulated values, and
expected values:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">D</span> <span class="o">=</span> <span class="n">pymc</span><span class="o">.</span><span class="n">discrepancy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_sim</span><span class="p">,</span> <span class="n">x_exp</span><span class="p">)</span>
</pre></div>
</div>
<p>For a dataset of size <span class="math">\(n\)</span> and an MCMC chain of length <span class="math">\(r\)</span>, this
implies that <code class="docutils literal"><span class="pre">x</span></code> is size <code class="docutils literal"><span class="pre">(n,)</span></code>, <code class="docutils literal"><span class="pre">x_sim</span></code> is size <code class="docutils literal"><span class="pre">(r,n)</span></code> and <code class="docutils literal"><span class="pre">x_exp</span></code>
is either size <code class="docutils literal"><span class="pre">(r,)</span></code> or <code class="docutils literal"><span class="pre">(r,n)</span></code>. A call to this function returns two
arrays of discrepancy values (simulated and observed), which can be passed to
the <code class="docutils literal"><span class="pre">discrepancy_plot</span></code> function in the <cite>Matplot</cite> module to generate a scatter
plot, and if desired, a <em>p</em> value:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">pymc</span><span class="o">.</span><span class="n">Matplot</span><span class="o">.</span><span class="n">discrepancy_plot</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;D&#39;</span><span class="p">,</span> <span class="n">report_p</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Additional optional arguments for <code class="docutils literal"><span class="pre">discrepancy_plot</span></code> are identical to other
PyMC plotting functions.</p>
<p class="rubric">Footnotes</p>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Asymptotic behaviour implies that the variance and the mean value of the sample
stays relatively constant over some arbitrary period.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/icon.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">7. Model checking and diagnostics</a><ul>
<li><a class="reference internal" href="#convergence-diagnostics">7.1. Convergence Diagnostics</a><ul>
<li><a class="reference internal" href="#informal-methods">7.1.1. Informal Methods</a></li>
<li><a class="reference internal" href="#formal-methods">7.1.2. Formal Methods</a></li>
</ul>
</li>
<li><a class="reference internal" href="#autocorrelation-plots">7.2. Autocorrelation Plots</a></li>
<li><a class="reference internal" href="#goodness-of-fit">7.3. Goodness of Fit</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="database.html"
                        title="previous chapter">6. Saving and managing sampling results</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="extending.html"
                        title="next chapter">8. Extending PyMC</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/modelchecking.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="extending.html" title="8. Extending PyMC"
             >next</a> |</li>
        <li class="right" >
          <a href="database.html" title="6. Saving and managing sampling results"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">PyMC 2.3.6 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2014, Christopher J. Fonnesbeck.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>